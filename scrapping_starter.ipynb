{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:.conda-10x]",
      "language": "python",
      "name": "conda-env-.conda-10x-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "scrapping_starter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaAZAdlorLbE",
        "colab_type": "text"
      },
      "source": [
        "## Web scrapping using python\n",
        "\n",
        "#### References\n",
        "1. [Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
        "2. [Web Scraping using Python](https://www.datacamp.com/community/tutorials/web-scraping-using-python)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVbCal7orLbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#$ python3 -m venv venv\n",
        "#$ . ./venv/bin/activate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSSrbd3xrLcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installing the required modules.\n",
        "!pip install requests BeautifulSoup4 fire  #Request is for performing HTTP requests and BeautifulSoup4 is for handling the HTML processing."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1OZDsg7rLdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing modules\n",
        "from requests import get\n",
        "from requests.exceptions import RequestException\n",
        "from contextlib import closing\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "\n",
        "import fire"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjYd4rEyrLeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile ../pyscrap_url.py\n",
        "\n",
        "def simple_get(url):\n",
        "    \"\"\"\n",
        "    Attempts to get the content at `url` by making an HTTP GET request.\n",
        "    If the content-type of response is some kind of HTML/XML, return the\n",
        "    text content, otherwise return None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with closing(get(url, stream=True)) as resp:\n",
        "            if is_good_response(resp):\n",
        "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "    except RequestException as e:\n",
        "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
        "        return None\n",
        "\n",
        "\n",
        "def is_good_response(resp):\n",
        "    \"\"\"\n",
        "    Returns True if the response seems to be HTML, False otherwise.\n",
        "    \"\"\"\n",
        "    content_type = resp.headers['Content-Type'].lower()\n",
        "    return (resp.status_code == 200 \n",
        "            and content_type is not None \n",
        "            and content_type.find('html') > -1)\n",
        "\n",
        "\n",
        "def log_error(e):\n",
        "    \"\"\"\n",
        "    It is always a good idea to log errors. \n",
        "    This function just prints them, but you can\n",
        "    make it do anything.\n",
        "    \"\"\"\n",
        "    print(e)\n",
        "    \n",
        "def get_elements(url, tag='',search={}, fname=None):\n",
        "    \"\"\"\n",
        "    Downloads a page specified by the url parameter\n",
        "    and returns a list of strings, one per tag element\n",
        "    \"\"\"\n",
        "    \n",
        "    if isinstance(url,str):\n",
        "        response = simple_get(url)\n",
        "    else:\n",
        "        #if already it is a loaded html page\n",
        "        response = url\n",
        "\n",
        "    if response is not None:\n",
        "        html = BeautifulSoup(response, 'html.parser')\n",
        "        \n",
        "        res = []\n",
        "        if tag:    \n",
        "            for li in html.select(tag):\n",
        "                for name in li.text.split('\\n'):\n",
        "                    if len(name) > 0:\n",
        "                        res.append(name.strip())\n",
        "                       \n",
        "                \n",
        "        if search:\n",
        "            soup = html            \n",
        "            \n",
        "            \n",
        "            r = ''\n",
        "            if 'find' in search.keys():\n",
        "                print('finding',search['find'])\n",
        "                soup = soup.find(**search['find'])\n",
        "                r = soup\n",
        "\n",
        "                \n",
        "            if 'find_all' in search.keys():\n",
        "                print('finding all of',search['find_all'])\n",
        "                r = soup.find_all(**search['find_all'])\n",
        "   \n",
        "            if r:\n",
        "                for x in list(r):\n",
        "                    if len(x) > 0:\n",
        "                        res.extend(x)\n",
        "            \n",
        "        return res\n",
        "\n",
        "    # Raise an exception if we failed to get any data from the url\n",
        "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
        "    \n",
        "    \n",
        "if get_ipython().__class__.__name__ == '__main__':\n",
        "    fire(get_elements)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCVKv3kecHVv",
        "colab_type": "text"
      },
      "source": [
        "Scraping the [The 100 most Influential Twitter users website\n",
        "](https://africafreak.com/100-most-influential-twitter-users-in-africa) to obtain the users twitter handle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Xy0BxijlrLfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aaedabb9-a762-43ba-faa1-b50bdc11ce6c"
      },
      "source": [
        "res = get_elements('https://africafreak.com/100-most-influential-twitter-users-in-africa',tag='h2')\n",
        "res\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100. Jeffrey Gettleman (@gettleman)',\n",
              " '99. Africa24 Media (@a24media)',\n",
              " '98. Scapegoat (@andiMakinana)',\n",
              " '97. Africa Check (@AfricaCheck)',\n",
              " '96. James Copnall (@JamesCopnall)',\n",
              " '95. Online Africa (@oafrica)',\n",
              " '94. Patrick Ngowi (@PatrickNgowi)',\n",
              " '93. DOS African Affairs (@StateAfrica)',\n",
              " '92. MoadowAJE (@Moadow)',\n",
              " '91. Brendan Boyle (@BrendanSAfrica)',\n",
              " '90. City of Tshwane (@CityTshwane)',\n",
              " '89. VISI Magazine (@VISI_Mag)',\n",
              " '88. andBeyond (@andBeyondSafari)',\n",
              " '87. This Is Africa (@ThisIsAfricaTIA)',\n",
              " '86. Sarah Carter (@sarzss)',\n",
              " '85. The EIU Africa team (@TheEIU_Africa)',\n",
              " '84. Investing In Africa (@InvestInAfrica)',\n",
              " '83. Barry Malone (@malonebarry)',\n",
              " '82. ARTsouthAFRICA (@artsouthafrica)',\n",
              " '81. Kahn Morbee (@KahnMorbee)',\n",
              " '80. Jamal Osman (@JamalMOsman)',\n",
              " '79. iamsuede™ (@iamsuede)',\n",
              " '78. Mike Stopforth (@mikestopforth)',\n",
              " '77. Equal Education (@equal_education)',\n",
              " '76. Tristan McConnell (@t_mcconnell)',\n",
              " '75. Kate Forbes (@forbeesta)',\n",
              " '74. Vanessa Raphaely (@hurricanevaness)',\n",
              " '73. Karen Allen (@BBCKarenAllen)',\n",
              " '72. Jax Panik (@jaxpanik)',\n",
              " '71. This Is Africa (@thisisafrica)',\n",
              " '70. Audi South Africa (@audisouthafrica)',\n",
              " '69. ONE (@ONEinAfrica)',\n",
              " '68. Hamza Mohamed (@Hamza_Africa)',\n",
              " '67. drew hinshaw (@drewfhinshaw)',\n",
              " '66. Rebecca Enonchong (@africatechie)',\n",
              " '65. marais (@cx73)',\n",
              " '64. George Ayittey (@ayittey)',\n",
              " '63. Mercedes-Benz SA (@MercedesBenz_SA)',\n",
              " '62. Africa Gathering (@africagathering)',\n",
              " '61. okayafrica (@okayafrica)',\n",
              " '60. Mary Harper (@mary_harper)',\n",
              " '59. Save the Rhino (@savetherhino)',\n",
              " '58. africa @pressfreedom (@africamedia_CPJ)',\n",
              " '57. TechCentral (@TechCentral)',\n",
              " '56. GautengGovt (@GautengProvince)',\n",
              " '55. Abdi Aynte (@Aynte)',\n",
              " '54. Daniel Howden (@daniel_howden)',\n",
              " '53. Ranger Diaries (@rangerdiaries)',\n",
              " '52. The Star (@TheStar_news)',\n",
              " '51. James Schneider (@schneiderhome)',\n",
              " '50. Afrinnovator (@Afrinnovator)',\n",
              " '49. theafricareport (@theafricareport)',\n",
              " '48. City of Joburg (@CityofJoburgZA)',\n",
              " '47. Think Africa Press (@ThinkAfricaFeed)',\n",
              " '46. Africa The Good News (@AfricaGoodNews)',\n",
              " '45. will ross (@willintune)',\n",
              " '44. CNBC Africa (@cnbcafrica)',\n",
              " '43. HowWeMadeItInAfrica (@MadeItInAfrica)',\n",
              " '42. Africa Research Inst (@AfricaResearch)',\n",
              " '41. FoodBlog Cape Town (@FoodBlogCT)',\n",
              " '40. Mbuyiseni Ndlozi (@MbuyiseniNdlozi)',\n",
              " '39. AfricaProgressPanel (@africaprogress)',\n",
              " '38. IFC Africa (@IFCAfrica)',\n",
              " '37. Henley Africa (@HenleyAfrica)',\n",
              " '36. The New Age (@The_New_Age)',\n",
              " '35. Geoffrey York (@geoffreyyork)',\n",
              " '34. Entrepreneur Mag SA (@Entrepreneur_SA)',\n",
              " '33. Forbes Africa (@forbesafrica)',\n",
              " '32. IEC South Africa (@IECSouthAfrica)',\n",
              " '31. Arthur Goldstuck (@art2gee)',\n",
              " '30. Jendayi E Frazer (@JendayiFrazer)',\n",
              " '29. Peter Greste (@PeterGreste)',\n",
              " '28. Disaster Operations (@NDOCKenya)',\n",
              " '27. Mo Ibrahim Fdn (@Mo_IbrahimFdn)',\n",
              " '26. Parliament of RSA (@ParliamentofRSA)',\n",
              " '25. Sandton City (@SandtonCity)',\n",
              " '24. African Union (@_AfricanUnion)',\n",
              " '23. Gert-Johan Coetzee (@gertjohan)',\n",
              " '22. David Smith (@SmithInAfrica)',\n",
              " '21. Ray Hartley (@hartleyr)',\n",
              " '20. Live Amp (@liveamp)',\n",
              " '19. Samsung South Africa (@SamsungSA)',\n",
              " '18. Bob Skinstad (@BobSkinstad)',\n",
              " '17. Camfed (@Camfed)',\n",
              " '16. andrew harding (@BBCAndrewH)',\n",
              " '15. Euphonik™♛ (@euphonik)',\n",
              " '14. Ulrich J van Vuuren (@UlrichJvV)',\n",
              " '13. John Robbie (@702JohnRobbie)',\n",
              " '12. Cricket South Africa (@OfficialCSA)',\n",
              " '11. MTV Base Africa (@MTVbaseAfrica)',\n",
              " '10. Computicket (@Computicket)',\n",
              " '9. loyiso gola (@loyisogola)',\n",
              " '8. 5FM (@5FM)',\n",
              " '7. mailandguardian (@mailandguardian)',\n",
              " '6. Helen Zille (@helenzille)',\n",
              " '5. Julius Sello Malema (@Julius_S_Malema)',\n",
              " '4. News24 (@News24)',\n",
              " '3. Jacob G. Zuma (@SAPresident)',\n",
              " '2. Gareth Cliff (@GarethCliff)',\n",
              " '1. Trevor Noah (@Trevornoah)',\n",
              " 'Subscribe to the list',\n",
              " 'Tweet about Africa?',\n",
              " 'Celebrate Wild Africa With Us!',\n",
              " 'You have successfully subscribed. Thank you!',\n",
              " '11 Comments']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn-TxSaZeUnG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "904f9082-164f-4a63-be30-d09bb85b4e5c"
      },
      "source": [
        "new = pd.DataFrame(res).head(100) #creating a datarame\n",
        "new\n",
        "#Data manipulation\n",
        "df1 = new[0].str.split('.', expand=True)\n",
        "df1.head(100)\n",
        "\n",
        "df2 = df1[1].str.split('(', expand=True)\n",
        "df2.head(100)\n",
        "\n",
        "df2[1] = df2[1].str.strip(')')\n",
        "df2.head(100)\n",
        "\n",
        "df2.columns = ['Twitter_name','Twitter_handle']\n",
        "df2\n",
        "\n",
        "\n",
        "Influencer_handle = df2['Twitter_handle']\n",
        "Influencer_handle\n",
        "\n",
        "Influencer_handle = df2['Twitter_handle'].astype(str).to_list()\n",
        "Influencer_handle\n",
        "\n",
        "reversed_list = Influencer_handle[::-1]\n",
        "reversed_list\n",
        "\n",
        "reversed_list.pop(2)  #removing None.\n",
        "reversed_list\n",
        "\n",
        "Top_10_influential = reversed_list[:10]\n",
        "\n",
        "\n",
        "#Converting into dataframe \n",
        "Top_10_influential_df = pd.DataFrame(Top_10_influential)\n",
        "\n",
        "#naming  the column\n",
        "Top_10_influential_df.columns = [\"Twitter__Handle\"]\n",
        "Top_10_influential_df"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Twitter__Handle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@Trevornoah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@GarethCliff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@News24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Julius_S_Malema</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@helenzille</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@mailandguardian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@5FM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@loyisogola</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@Computicket</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@MTVbaseAfrica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Twitter__Handle\n",
              "0       @Trevornoah\n",
              "1      @GarethCliff\n",
              "2           @News24\n",
              "3  @Julius_S_Malema\n",
              "4       @helenzille\n",
              "5  @mailandguardian\n",
              "6              @5FM\n",
              "7       @loyisogola\n",
              "8      @Computicket\n",
              "9    @MTVbaseAfrica"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9WOl3oI6VYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9b8d4b9d-9e08-4fee-c900-40b1837cf0b7"
      },
      "source": [
        "#Saving file\n",
        "\n",
        "Top_10_influential_df.to_csv('Top 10 Influencers Twitter handle.csv',index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"Top 10 Influencers Twitter handle.csv\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d7fcd2fb-64c3-425a-bf03-141799f63958\", \"Top 10 Influencers Twitter handle.csv\", 140)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIIDnXLFdt49",
        "colab_type": "text"
      },
      "source": [
        "Scrapping the [The website of top government officials responding to Coronavirus in East Africa](https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa) and obtain their twitter handles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyDjOcqjrLgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Obtaining the url\n",
        "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
        "response = simple_get(url)\n",
        "\n",
        "res = get_elements(response, search={'find_all':{'class_':'twitter-tweet'}})\n",
        "res\n",
        "#Obtaining the specific strings with the twitter name and handle.\n",
        "my_stringtags = []\n",
        "for tag in res:\n",
        "  if tag.string != None:\n",
        "    my_stringtags.append(tag.string)\n",
        "my_stringtags\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjEufMcUTClu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "9395e171-8e71-4fbe-de67-e4ab80801bca"
      },
      "source": [
        "#Extracting the specific twitter handles.\n",
        "import re\n",
        "handleregex = re.compile(r'@[a-zA-Z0-9_]{0,15}')\n",
        "tags = ''.join(my_stringtags)\n",
        "govt_official = handleregex.findall(tags)\n",
        "govt_handle = pd.DataFrame(govt_official)\n",
        "govt_handle\n",
        "\n",
        "##Data manipulation\n",
        "#govt_handle[0] = govt_handle[0].str.strip('@')\n",
        "#govt_handle.head(100)\n",
        "\n",
        "govt_handle.columns = ['Twitter_handle']\n",
        "govt_handle\n",
        "\n",
        "Top_officials = govt_handle['Twitter_handle'].astype(str).to_list()\n",
        "Top_officials\n",
        "\n",
        "Top_10_govt_official = Top_officials[:10]\n",
        "\n",
        "Top_10_govt_official_df = pd.DataFrame(Top_10_govt_official)\n",
        "Top_10_govt_official_df.columns = [\"Twitter_Handle\"]\n",
        "Top_10_govt_official_df\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Twitter_Handle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@EswatiniGovern1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@MalawiGovt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@hagegeingob</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@FinanceSC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@PresidencyZA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@mohzambia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@edmnangagwa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@MinSantedj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@hawelti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@StateHouseKenya</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Twitter_Handle\n",
              "0  @EswatiniGovern1\n",
              "1       @MalawiGovt\n",
              "2      @hagegeingob\n",
              "3        @FinanceSC\n",
              "4     @PresidencyZA\n",
              "5        @mohzambia\n",
              "6      @edmnangagwa\n",
              "7       @MinSantedj\n",
              "8          @hawelti\n",
              "9  @StateHouseKenya"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCJKauS7cZMy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "520c22d0-59a3-4754-ec83-513b900d6c2b"
      },
      "source": [
        "#Saving file\n",
        "Top_10_govt_official_df.to_csv('Top 10 govt officials Twitter Handle.csv',index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"Top 10 govt officials Twitter Handle.csv\")"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f0d63fb7-a448-4a74-9bf5-3b3d86715957\", \"Top 10 govt officials Twitter Handle.csv\", 144)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UY21zJkrLii",
        "colab_type": "text"
      },
      "source": [
        "## Web scrapping using bash script\n",
        "If the web site has a quite simple HTML, you can easily use curl to perform the request and then extract the needed values using bash commands grep, cut , sed, ..\n",
        "\n",
        "This tutorial is adapted from [this](https://medium.com/@LiliSousa/web-scraping-with-bash-690e4ee7f98d) medium article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hPWxJRL4SZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9b6f06c8-6347-4563-c973-6a1318a9e5b3"
      },
      "source": [
        "%%bash \n",
        "\n",
        "# curl the page and save content to tmp_file\n",
        "url = \"https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa\"\n",
        "curl -X GET $url -o tmp_file\n",
        "\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "# write headers to CSV file\n",
        "echo \"Name, twitter_id\" >> extractData.csv\n",
        "n=\"1\"\n",
        "while [ $n -lt 2 ]\n",
        "do\n",
        "  \n",
        "  #get title\n",
        "  title=$(cat tmp_file | grep \"class=\\\"twitter-tweet\\\"\" | cut -d ';' -f1 )\n",
        "  echo $title\n",
        "  #get author\n",
        "  twitter_id=$(cat tmp_file |grep -A1 \"class=\\\"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\\\"\" | tail -1)\n",
        "\n",
        "  echo \"$title, $twitter_id\" >> extractData.csv\n",
        "  echo \"$title, $twitter_id\"\n",
        "    \n",
        "  n=$[$n+1\n",
        "\n",
        "done"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            ", \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bash: line 3: url: command not found\n",
            "curl: no URL specified!\n",
            "curl: try 'curl --help' or 'curl --manual' for more information\n",
            "cat: tmp_file: No such file or directory\n",
            "cat: tmp_file: No such file or directory\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}